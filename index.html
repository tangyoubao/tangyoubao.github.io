
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="keywords" content="Youbao Tang" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="style.css" type="text/css" />
<!--<link rel="shortcut icon" href="fig/cripac.png">-->
<title>Youbao Tang's Homepage</title>
</head>
<body>
<div id="layout-content">

<script type="text/javascript">
<!--
// Toggle Display of BibTeX
function toggleBibtex(articleid) {
  var bib = document.getElementById(articleid);
  // Toggle 
    if(bib.style.display == "none") {
      bib.style.display = "";
    }
    else {
      bib.style.display = "none";
    }
}
-->
</script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-40926388-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>

<table class="imgtable"><tr><td>
<img src="fig/tangyoubao.jpg" alt="alt text" width="137px" height="177px" /> &nbsp;</td>
<td align="left">

<div id="toptitle"> 
  <h1>
  <a href="http://tangyoubao.github.io/">Youbao Tang</a> &nbsp; 唐有宝
  </h1>
</div>

<p>
PostDoc in <a href="https://www.nih.gov/">National Institutes of Health (NIH)</a>, supervised by <a href="https://clinicalcenter.nih.gov/about/SeniorStaff/ronald_summers.html">Dr. Ronald Summers</a> and <a href="http://www.cs.jhu.edu/~lelu/">Dr. Le Lu</a>
<br />
<br />

Email: <a href="mailto:tybxiaobao@gmail.com">tybxiaobao at gmail.com</a><br />
<!--Address: 10 CENTER DR RM 1C224 BETHESDA MD 20892<br /><br />-->
[<a href= "fig/cv.pdf">CV</a>]<br />
</p>
</td></tr></table>


  <td align="bottom" colspan="2" style="margin-top:2px;">
					<h2>News</h2>
					<div style="height: 120px; overflow: auto;">
				    <ul style="margin-left:2px; padding-left:20px; margin-top:0px">
						<li>
  <p>
      2017-07-02: Our paper "<i>Salient Object Detection with Chained Multi-Scale Fully Convolutional Network</i>" is accepted by ACMMM 2017.
  </p>
</li>
  
  <li>
  <p>
      2017-03-03: One paper submitted to IEEE TMM is given MAJOR REVISION (RQ) decision.
  </p>
</li>
					    <li>
  <p>
      2017-07-02: Our paper "<i>Salient Object Detection with Chained Multi-Scale Fully Convolutional Network</i>" is accepted by ACMMM 2017.
  </p>
</li>
  
  <li>
  <p>
      2017-03-03: One paper submitted to IEEE TMM is given MAJOR REVISION (RQ) decision.
  </p>
</li>
					    <li>
  <p>
      2017-07-02: Our paper "<i>Salient Object Detection with Chained Multi-Scale Fully Convolutional Network</i>" is accepted by ACMMM 2017.
  </p>
</li>
  
  <li>
  <p>
      2017-03-03: One paper submitted to IEEE TMM is given MAJOR REVISION (RQ) decision.
  </p>
</li>
</ul>
					</div>
 				</td>


<h2>
  Biography 
</h2>
<ul>

<li>
  <p>
    I'm Youbao TANG from China. Now I am a postdoc researcher in <a href="https://www.nih.gov/">National Institutes of Health (NIH)</a>, US. My tutors are <a href="https://clinicalcenter.nih.gov/about/SeniorStaff/ronald_summers.html">Dr. Ronald Summers</a> and <a href="http://www.cs.jhu.edu/~lelu/">Dr. Le Lu</a>.
  </p>
</li>

<li>
  <p>
    I received the Bachelor, Master and Doctor degree under Prof. <a href="http://homepage.hit.edu.cn/pages/wuxiangqian">Xiangqian Wu</a>'s advice in <a href="http://cs.hit.edu.cn/">School of Computer Science and Technology</a> from <a href="http://www.hit.edu.cn/">Harbin Institute of Technology (HIT)</a>. 
  </p>
</li>

<li>
  <p>
     My research interests include computer vision, medical image processing, deep learning.
  </p>
</li>
</ul>

<h2>Selected Publications (<a href="https://scholar.google.com/citations?user=udx2IT0AAAAJ&hl=zh-CN">full list</a>)</h2> 
  
<ul>
<li>
Salient Object Detection with Chained Multi-Scale Fully Convolutional Network <br />
<b>Youbao Tang</b>, Xiangqian Wu <br />
<i>ACM Multimedia Conference</i> (<b>ACMMM</b>), 2017. <br />
</li>

<li>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828014">Scene Text Detection and Segmentation via Cascaded Convolution Neural Networks</a> <br />
<b>Youbao Tang</b>, Xiangqian Wu <br />
<i>IEEE Transactions on Image Processing</i> (<b>TIP</b>), 2017. <br />
</li>

<li>
<a href="https://arxiv.org/pdf/1608.05186.pdf">Saliency Detection via Combining Region-level and Pixel-level Predictions with CNNs</a> <br />
<b>Youbao Tang</b>, Xiangqian Wu <br />
<i>European Conference on Computer Vision</i> (<b>ECCV</b>), 2016. <br />
</li>
  
<li>
<a href="https://arxiv.org/pdf/1608.05177.pdf">Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection</a> <br />
<b>Youbao Tang</b>, Xiangqian Wu, Wei Bu <br />
<i>ACM Multimedia Conference</i> (<b>ACMMM</b>), 2016. <br />
</li>

<li>
<a href="http://dl.acm.org/citation.cfm?id=2806287">Saliency Detection Based on Graph-Structural Agglomerative Clustering</a> <br />
<b>Youbao Tang</b>, Xiangqian Wu, Wei Bu <br />
<i>ACM Multimedia Conference</i> (<b>ACMMM</b>), 2015. <br />
</li>
  
<li>
<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716030">Offline Text-Independent Writer Identification Based on Scale Invariant Feature Transform</a> <br />
Xiangqian Wu, <b>Youbao Tang</b>, Wei Bu <br />
<i>IEEE Transactions on Information Forensics and Security</i> (<b>TIFS</b>), 2014. <br />
</li>

</ul>
 <!--
<ul>

<li>
<a href="https://arxiv.org/abs/1704.01719"> Beyond triplet loss: a deep quadruplet network for person re-identification,</a> <br />
<b>Weihua Chen</b>, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang <br />
<i>The Conference on Computer Vision and Pattern Recognition</i> (<b>CVPR Spotlight</b>), 2017. <br />
</li>
[<a href="https://arxiv.org/pdf/1704.01719.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('Quadruplet_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('Quadruplet')" target="_self">Bibtex</a>]
[<a href="https://arxiv.org/abs/1704.01719">DOI</a>]
<div class="blockcontent" id="Quadruplet_abstract" style="display:none"> 
  <p style="font-size:16px">
          Person re-identification (ReID) is an important task in wide area video surveillance which focuses on identifying people across different cameras. Recently, deep learning networks with a triplet loss become a common framework for person ReID. However, the triplet loss pays main attentions on obtaining correct orders on the training set. It still suffers from a weaker generalization capability from the training set to the testing set, thus resulting in inferior performance. In this paper, we design a quadruplet loss, which can lead to the model output with a larger inter-class variation and a smaller intra-class variation compared to the triplet loss. As a result, our model has a better generalization ability and can achieve a higher performance on the testing set. In particular, a quadruplet deep network using a margin-based online hard negative mining is proposed based on the quadruplet loss for the person ReID. In extensive experiments, the proposed network outperforms most of the state-of-the-art algorithms on representative datasets which clearly demonstrates the effectiveness of our proposed method.
      </p>
</div>

<div class="blockcontent" id="Quadruplet" style="display:none"> 
<pre>
@inproceedings{Chen/cvpr2017,
 title = {Beyond triplet loss: a deep quadruplet network for person re-identification},
 author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
 booktitle = {The Conference on Computer Vision and Pattern Recognition},
 year = {2017}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://arxiv.org/abs/1607.05369"> A Multi-task Deep Network for Person Re-identification,</a> <br />
<b>Weihua Chen</b>, Xiaotang Chen, Jianguo Zhang, Kaiqi Huang <br />
<i>The Thirty-First AAAI Conference on Artificial Intelligence</i> (<b>AAAI Oral</b>), 2017. <br />
</li>
[<a href="https://arxiv.org/pdf/1607.05369v3.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('MTDnet_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('MTDnet')" target="_self">Bibtex</a>]
[<a href="https://arxiv.org/abs/1607.05369">DOI</a>]
[<a href="https://github.com/cwhgn/MTDnet">Code</a>]
<div class="blockcontent" id="MTDnet_abstract" style="display:none"> 
  <p style="font-size:16px">
          Person re-identification (ReID) focuses on identifying people across different scenes in video surveillance, which is usually formulated as a binary classification task or a ranking task in current person ReID approaches. In this paper, we take both tasks into account and propose a multi-task deep network (MTDnet) that makes use of their own advantages and jointly optimize the two tasks simultaneously for person ReID. To the best of our knowledge, we are the first to integrate both tasks in one network to solve the person ReID. We show that our proposed architecture significantly boosts the performance. Furthermore, deep architecture in general requires a sufficient dataset for training, which is usually not met in person ReID. To cope with this situation, we further extend the MTDnet and propose a cross-domain architecture that is capable of using an auxiliary set to assist training on small target sets. In the experiments, our approach outperforms most of existing person ReID algorithms on representative datasets including CUHK03, CUHK01, VIPeR, iLIDS and PRID2011, which clearly demonstrates the effectiveness of the proposed approach.
      </p>
</div>

<div class="blockcontent" id="MTDnet" style="display:none"> 
<pre>
@inproceedings{Chen/aaai2017,
 title = {A Multi-task Deep Network for Person Re-identification},
 author={Chen, Weihua and Chen, Xiaotang and Zhang, Jianguo and Huang, Kaiqi},
 booktitle = {The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17)},
 year = {2017}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://arxiv.org/abs/1502.03532"> An Equalised Global Graphical Model-Based Approach for Multi-Camera Object Tracking,</a> <br />
<b>Weihua Chen</b>, Lijun Cao, Xiaotang Chen, Kaiqi Huang <br />
<i>IEEE Transactions  on Circuits and Systems for Video Technology</i> (<b>TCSVT</b>), 2016. <br />
</li>
[<a href="http://arxiv.org/pdf/1502.03532v1.pdf">PDF</a>]
[<a href="javascript:toggleBibtex('TCSVT15_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('TCSVT15')" target="_self">Bibtex</a>]
[<a href="http://arxiv.org/abs/1502.03532">DOI</a>]
[<a href="https://github.com/cwhgn/EGTracker">Code</a>]
[<a href="http://youtu.be/GZ2u2tvzgi4">Demos</a>]
<div class="blockcontent" id="TCSVT15_abstract" style="display:none"> 
  <p style="font-size:16px">
          Multi-camera non-overlapping visual object tracking system typically consists of two tasks: single camera object tracking and inter-camera object tracking. Since the state-of-the-art approaches are yet not perform perfectly in real scenes, the errors in single camera object tracking module would propagate into the module of inter-camera object tracking, resulting much lower overall performance. In order to address this problem, we develop an approach that jointly optimise the single camera object tracking and inter-camera object tracking in an equalised global graphical model. Such an approach has the advantage of guaranteeing a good overall tracking performance even when there are limited amount of false tracking in single camera object tracking. Besides, the similarity metrics used in our approach improve the compatibility of the metrics used in the two different tasks. Results show that our approach achieve the state-of-the-art results in multi-camera non-overlapping tracking datasets.
      </p>
</div>

<div class="blockcontent" id="TCSVT15" style="display:none"> 
<pre>
@article{chen2015equalised,
  title={An equalised global graphical model-based approach for multi-camera object tracking},
  author={Chen, Weihua and Cao, Lijun and Chen, Xiaotang and Huang, Kaiqi},
  journal={arXiv preprint arXiv:1502.03532},
  year={2015}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7025472&tag=1"> A Novel Solution For Multi-Camera Object Tracking, </a> <br />
<b>Weihua Chen</b>, Lijun Cao, Xiaotang Chen, Kaiqi Huang <br />
<i>IEEE International Conference on Image Processing</i> (<b>ICIP</b>), 2014. <br />
</li>
[<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7025472">PDF</a>]
[<a href="javascript:toggleBibtex('ICIP14_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICIP14')" target="_self">Bibtex</a>]
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7025472&tag=1">DOI</a>]
<div class="blockcontent" id="ICIP14_abstract" style="display:none"> 
  <p style="font-size:16px">
          The traditional multi-camera object tracking contains two steps: single camera object tracking (SCT) and inter-camera object tracking (ICT). The ICT performance strongly relies on the great results of SCT. In practice, most of current SCT methods are unperfect and products much more fragments. In this paper, a novel solution using a global tracklet association is proposed, which can provide a good ICT performance when the SCT results are not perfect. The proposed solution is also available in non-overlapping views through a new tracklet representation and experiments shows the effectiveness of the proposed novel solution in real scene.
      </p>
</div>

<div class="blockcontent" id="ICIP14" style="display:none"> 
<pre>
@inproceedings{chen2014novel,
  title={A novel solution for multi-camera object tracking},
  author={Chen, Weihua and Cao, Lijun and Chen, Xiaotang and Huang, Kaiqi},
  booktitle={Image Processing (ICIP), 2014 IEEE International Conference on},
  pages={2329--2333},
  year={2014},
  organization={IEEE}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6755888&tag=1"> An Adaptive Combination of Multiple Features for Robust Tracking in Real Scene, </a> <br />
<b>Weihua Chen</b>, Lijun Cao, Junge Zhang, Kaiqi Huang <br />
<i>IEEE International Conference on  Computer Vision Workshop on Visual Object Tracking Challenge</i> (<b>ICCVW Oral</b>), 2013. <br />
</li>
[<a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6755888">PDF</a>]
[<a href="javascript:toggleBibtex('ICCVW13_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICCVW13')" target="_self">Bibtex</a>]
[<a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6755888&tag=1">DOI</a>]
<div class="blockcontent" id="ICCVW13_abstract" style="display:none"> 
  <p style="font-size:16px">
          Real scene video surveillance always involves low resolutions, lack of illumination or cluttered environments, which leads to insufficiency of discriminative details for the target. In this situation, discrimination based tracking methods could fail. To address this problem, this paper presents an adaptive multi-feature integration method in terms of feature invariance, which can evaluate the stability of features in sequential frames. The adaptive integrated feature (AIF) is consisted of several features with dynamic weights, which describe the degree of invariance of each single feature. An incremental principal component analysis (IPCA) adjusted by the accuracy of tracking results is used to update the adaptive integrated feature, and partially avoids the problem of "updating dilemma", which is common in most of adaptive updating methods. Experiments on pedestrian tracking demonstrate the proposed approach is effective and shows improved performance compared with several state-of-the-art methods in real surveillance scenes.
      </p>
</div>

<div class="blockcontent" id="ICCVW13" style="display:none"> 
<pre>
@inproceedings{chen2013adaptive,
  title={An adaptive combination of multiple features for robust tracking in real scene},
  author={Chen, Weihua and Cao, Lijun and Zhang, Junge and Huang, Kaiqi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={129--136},
  year={2013}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://dl.acm.org/citation.cfm?id=2632874"> A Smart Meeting Management  System With Video Based Seat Detection, </a> <br />
Lijun Cao, <b>Weihua Chen</b>, Xu Zhang, Kaiqi Huang <br />
<i>ACM International Conference on Internet Multimedia  Computing and Service</i> (<b>ICIMCS</b>), 2014. <br />
</li>
[<a href="http://delivery.acm.org/10.1145/2640000/2632874/p232-cao.pdf?ip=159.226.179.184&id=2632874&acc=ACTIVE%20SERVICE&key=33E289E220520BFB.949A0B1AADF887FF.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=478662293&CFTOKEN=48147274&__acm__=1423729651_3a463a4fc3ef22839c7c190ac700ef4e">PDF</a>]
[<a href="javascript:toggleBibtex('ICIMCS14a_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICIMCS14a')" target="_self">Bibtex</a>]
[<a href="http://dl.acm.org/citation.cfm?id=2632874">DOI</a>]
<div class="blockcontent" id="ICIMCS14a_abstract" style="display:none"> 
  <p style="font-size:16px">
          Nowadays intelligent meeting management is attracting more and more attention. Compared with most existing smart meeting systems which care more about recording talks during meeting in small meeting room, this paper describes an intelligent meeting management system for large auditorium. A cascade empty seat detection algorithm is embedded in the system to acquire the statuses of each seat. Empty seats are labeled step by step according to the extent of occlusions. Combined with attendee information input in advance, the system could provide people with the seated condition and corresponding information of attendees during the meeting. Experimental results demonstrate that the precision of seat＼s statuses detection could achieve 99.8%.
      </p>
</div>

<div class="blockcontent" id="ICIMCS14a" style="display:none"> 
<pre>
@inproceedings{cao2014smart,
  title={A smart meeting management system with video based seat detection},
  author={Cao, Lijun and Chen, Weihua and Zhang, Xu and Huang, Kaiqi},
  booktitle={Proceedings of International Conference on Internet Multimedia Computing and Service},
  pages={232},
  year={2014},
  organization={ACM}
}
</pre>
</div>
<br /><br />

<li>
<a href="http://dl.acm.org/citation.cfm?id=2632875"> License Plate Localization With Efficient Markov Chain Monte Carlo, </a> <br />
Lijun Cao, Xu Zhang, <b>Weihua Chen</b>, Kaiqi Huang <br />
<i>ACM International Conference on Internet Multimedia  Computing and Service</i> (<b>ICIMCS</b>), 2014. <br />
</li>
[<a href="http://delivery.acm.org/10.1145/2640000/2632875/p295-cao.pdf?ip=159.226.179.184&id=2632875&acc=ACTIVE%20SERVICE&key=33E289E220520BFB.949A0B1AADF887FF.4D4702B0C3E38B35.4D4702B0C3E38B35&CFID=478662293&CFTOKEN=48147274&__acm__=1423729709_b0f999fdc4df3e522a521c2999917fb0">PDF</a>]
[<a href="javascript:toggleBibtex('ICIMCS14b_abstract')" target="_self">Abstract</a>]
[<a href="javascript:toggleBibtex('ICIMCS14b')" target="_self">Bibtex</a>]
[<a href="http://dl.acm.org/citation.cfm?id=2632875">DOI</a>]
<div class="blockcontent" id="ICIMCS14b_abstract" style="display:none"> 
  <p style="font-size:16px">
          This paper presents a novel efficient Markov Chain Monte Carlo (MCMC) method for License Plate (LP) localization. The proposed method formulates the LP image feature and prior knowledge into a unified Bayesian framework. Then the localization problem is derived as a maximizing-a-posterior (MAP) problem, which integrates color, edge and character feature of LP. We propose an efficient MCMC method, taking integrated local geometrical likelihood as proposal probability to make the inference feasible. The experimental results on real dataset are very promising in terms of detection rate and localization accuracy.
      </p>
</div>

<div class="blockcontent" id="ICIMCS14b" style="display:none"> 
<pre>
@inproceedings{cao2014license,
  title={License Plate Localization With Efficient Markov Chain Monte Carlo},
  author={Cao, Lijun and Zhang, Xu and Chen, Weihua and Huang, Kaiqi},
  booktitle={Proceedings of International Conference on Internet Multimedia Computing and Service},
  pages={295},
  year={2014},
  organization={ACM}
}
</pre>
</div>
<br /><br />
</ul>

<h2>Activity</h2>
<ul>
<li>
Designing a multi-camera multi-object tracking benchmark for <a href="http://mct.idealtest.org/">the Multi-Camera Object Tracking (<b>MCT</b>) Challenge </a> in <a href="http://eccv2014.org/">ECCV 2014</a> <a href="http://www.vs-re-id-2014.org/">Workshop on Visual Surveillance and Re-identification</a>, including:<br />
<a href="http://mct.idealtest.org/Datasets.html">NLPR_MCT Dataset</a> : A dataset consisting of four subsets for non-overlapping multi-camera tracking;<br /> 
<a href="http://mct.idealtest.org/Evaluation.html">MCTA</a> : A criterion to evaluate the performance of multi-camera multi-object tracking;<br /> 
<a href="http://mct.idealtest.org/Evaluation.html">MCTA Evaluation Toolkit </a> : An evaluation kit for computing Multi-Camera Tracking Accuracy (MCTA).<br /> 
</li>
</ul>

<p>
  <a href="http://english.ia.cas.cn/"><img src="fig/casia.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.nlpr.ia.ac.cn/"><img src="fig/nlpr.jpg" alt="alt text" width="50px" height="50px"/></a>

  <a href="http://www.cripac.ia.ac.cn/"><img src="fig/cripac.png" alt="alt text" width="50px" height="50px"/></a> 

  <a href="http://www.bjtu.edu.cn/"><img src="fig/bjtu.jpg" alt="alt text" width="50px" height="50px"/></a> 
</p>
-->
<h2>Reviews</h2>
<ul>
<li>
<p>
<a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">IEEE Transactions on Information Forensics and Security</a>
</p>
</li>

<li>
<p>
<a href="https://www.journals.elsevier.com/knowledge-based-systems/">Knowledge-Based Systems</a>
</p>
</li>

<li>
<p>
<a href="http://www.tandfonline.com/action/journalInformation?journalCode=teta20">Journal of Experimental & Theoretical Artificial Intelligence</a>
</p>
</li>

<li>
<p>
<a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=4149689">IET Image Processing</a>
</p>
</li>
</ul>
 
  
<br>
<!-- BEGIN: Powered by Supercounters.com -->
<center><script type="text/javascript" src="//widget.supercounters.com/ssl/flag.js"></script><script type="text/javascript">sc_flag(1425375,"FFFFFF","000000","cccccc",2,10,0,0)</script><br><noscript><a href="http://www.supercounters.com/">Flag Counter</a></noscript>
</center>
<!-- END: Powered by Supercounters.com -->
 
  
<div id="footer">
<div id="footer-text">
</br>Last updated at 2017-07-02 by Youbao Tang.
</div>
</div>
</div>


</body>
</html>
